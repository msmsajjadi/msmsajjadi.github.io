<!-- ## Mehdi S. M. Sajjadi -->

## Mehdi S. M. Sajjadi

I am a Tech Lead at [Google DeepMind](https://deepmind.google/), where my work focuses on 3D scene understanding and deep generative models. With over a decade of experience in the field, I am passionate about building systems that can perceive and generate the world in 3D. Previously, I completed my PhD at the [Max Planck Institute for Intelligent Systems](https://is.mpg.de/) with [Bernhard SchÃ¶lkopf](https://is.mpg.de/~bs), supported by a fellowship from [ETH ZÃ¼rich CLS](https://learning-systems.org/). I earned my MSc with distinction in Computer Science & Math from the University of Hamburg.

## Publications (<a href="https://scholar.google.com/citations?hl=de&user=rHF25YEAAAAJ" target="_blank" rel="noopener noreferrer">Google Scholar</a>)

- ðŸ”¥ <a href="https://arxiv.org/abs/2412.15212" target="_blank" rel="noopener noreferrer">Scaling 4D Representations</a>
  - <a href="https://github.com/google-deepmind/representations4d" target="_blank" rel="noopener noreferrer">Code & Models</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2502.07001" target="_blank" rel="noopener noreferrer">From Image to Video: An Empirical Study of Diffusion Representations</a> (ICCV 2025 Highlight)

- ðŸ”¥ <a href="https://arxiv.org/abs/2509.10156" target="_blank" rel="noopener noreferrer">LayerLock: Non-collapsing Representation Learning with Progressive Freezing</a> (ICCV 2025)

- ðŸ”¥ <a href="https://arxiv.org/abs/2504.05579" target="_blank" rel="noopener noreferrer">TAPNext: End-to-End Tracking Any Point as Next Token Prediction</a> (ICCV 2025)
  - <a href="https://tap-next.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2412.14294" target="_blank" rel="noopener noreferrer">TRecViT: A Recurrent Video Transformer </a>
  - <a href="https://github.com/google-deepmind/trecvit" target="_blank" rel="noopener noreferrer">Github</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2505.00209" target="_blank" rel="noopener noreferrer">Direct Motion Models for Assessing Generated Videos</a> (ICML 2025)
  - <a href="https://trajan-paper.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2411.05927" target="_blank" rel="noopener noreferrer">Moving Off-the-Grid: Scene-Grounded Video Representations</a> (NeurIPS 2024 **Spotlight, top 2%**)

- <a href="https://arxiv.org/abs/2310.06020" target="_blank" rel="noopener noreferrer">DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</a> (ICLR 2024 **Spotlight, top 5%**)
  - <a href="https://dyst-paper.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2211.14306" target="_blank" rel="noopener noreferrer">RUST: Latent Neural Scene Representations from Unposed Imagery</a> (CVPR 2023 **Highlight, top 2.5%**)
  - <a href="https://rust-paper.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2303.03378" target="_blank" rel="noopener noreferrer">PaLM-E: An Embodied Multimodal Language Model</a> (ICML 2023)
  - <a href="https://palm-e.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2306.08068" target="_blank" rel="noopener noreferrer">DORSal: Diffusion for Object-centric Representations of Scenes et al.</a> (ICLR 2024)
  - <a href="https://www.sjoerdvansteenkiste.com/dorsal/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2305.18890" target="_blank" rel="noopener noreferrer">Sensitivity of Slot-Based Object-Centric Models to their Number of Slots</a>

- <a href="https://arxiv.org/abs/2304.00947" target="_blank" rel="noopener noreferrer">RePAST: Relative Pose Attention Scene Representation Transformer</a> (CVPR 2023 Workshops 3DMV & T4V Spotlight)

- <a href="https://arxiv.org/abs/2203.11194" target="_blank" rel="noopener noreferrer">Test-time adaptation with slot-centric models</a> (ICML 2023, NeurIPS 2022 Workshops MetaLearn & DistShift)
  - <a href="https://mihirp1998.github.io/project_pages/slottta" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://openreview.net/forum?id=nk_nSogsrZL" target="_blank" rel="noopener noreferrer">Spatial Symmetry in Slot Attention</a> (ICML 2023, NeurIPS 2022 Workshop NeurReps)

- <a href="https://arxiv.org/abs/2206.06922v1" target="_blank" rel="noopener noreferrer">Object Scene Representation Transformer</a> (NeurIPS 2022)
  - <a href="https://osrt-paper.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/2111.13152" target="_blank" rel="noopener noreferrer">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations</a> (CVPR 2022)
  - <a href="https://srt-paper.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2112.00724" target="_blank" rel="noopener noreferrer">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</a> (CVPR 2022 **Oral**)
  - <a href="https://m-niemeyer.github.io/regnerf/index.html" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2203.03570" target="_blank" rel="noopener noreferrer">Kubric</a> (CVPR 2022)
  - <a href="https://github.com/google-research/kubric" target="_blank" rel="noopener noreferrer">Github</a>

- <a href="https://arxiv.org/abs/2111.13260" target="_blank" rel="noopener noreferrer">NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes</a> (TMLR 2022)
  - <a href="https://nesf3d.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/2008.02268" target="_blank" rel="noopener noreferrer">NeRF in the Wild</a> (CVPR 2021 **Oral**)
  - <a href="https://nerf-w.github.io/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://arxiv.org/abs/1903.12436" target="_blank" rel="noopener noreferrer">From Variational to Deterministic Autoencoders</a> (ICLR 2020)

- ðŸ”¥ <a href="https://arxiv.org/abs/1806.00035" target="_blank" rel="noopener noreferrer">Assessing Generative Models via Precision and Recall</a> (NeurIPS 2018 & ICML 2018 workshop TADGM **Oral** & **Best Poster Award** at Bosch AI Con 2018)
  - <a href="https://github.com/msmsajjadi/precision-recall-distributions" target="_blank" rel="noopener noreferrer">GitHub</a>

- <a href="https://arxiv.org/abs/1807.07930" target="_blank" rel="noopener noreferrer">Photorealistic Video Super-Resolution with Enhanced Temporal Consistency</a> (ECCV 2018 workshop PIRM)

- <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Tae_Hyun_Kim_Spatio-temporal_Transformer_Network_ECCV_2018_paper.html" target="_blank" rel="noopener noreferrer">Spatio-temporal Transformer Network for Video Restoration</a> (ECCV 2018)

- <a href="https://arxiv.org/abs/1802.04374" target="_blank" rel="noopener noreferrer">Tempered Adversarial Networks</a> (ICML 2018 **Oral** & ICLR 2018 workshop)

- <a href="https://arxiv.org/abs/1801.04590" target="_blank" rel="noopener noreferrer">Frame-Recurrent Video Super-Resolution</a> (CVPR 2018)
  - <a href="https://github.com/msmsajjadi/frvsr" target="_blank" rel="noopener noreferrer">GitHub</a>

- ðŸ”¥ <a href="https://arxiv.org/abs/1612.07919" target="_blank" rel="noopener noreferrer">EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</a> (ICCV 2017 **Oral, top 2%**)
  - <a href="https://webdav.tuebingen.mpg.de/pixel/enhancenet/" target="_blank" rel="noopener noreferrer">Project website</a>
  - <a href="https://github.com/msmsajjadi/EnhanceNet-Code" target="_blank" rel="noopener noreferrer">GitHub</a>
  - Our work has been covered in the press:
    - <a href="https://ei.is.tuebingen.mpg.de/news/from-small-to-not-so-pixel-perfect-large" target="_blank" rel="noopener noreferrer">Press release</a>
    - <a href="https://www.dailymail.co.uk/sciencetech/article-5056501/Blade-Runner-AI-make-grainy-images-razor-sharp.html#" target="_blank" rel="noopener noreferrer">Daily Mail</a>
    - <a href="https://petapixel.com/2017/11/01/photo-enhancement-starting-get-crazy/" target="_blank" rel="noopener noreferrer">PetaPixel</a>
    - <a href="https://techxplore.com/news/2017-10-small-pixel-perfect-large.html" target="_blank" rel="noopener noreferrer">TechXplore</a>
    - <a href="https://www.golem.de/news/forschung-maschinelles-lernsystem-verbessert-aufloesung-von-bildern-1710-130864.html" target="_blank" rel="noopener noreferrer">Golem (German)</a>
    - <a href="https://www.google.com/search?q=EnhanceNet&tbm=nws" target="_blank" rel="noopener noreferrer">...and many more</a>

- <a href="https://ei.is.tuebingen.mpg.de/publications/sajkohschhir16" target="_blank" rel="noopener noreferrer">Depth Estimation Through a Generative Model of Light Field Synthesis</a> (GCPR 2016)
  - <a href="https://webdav.tue.mpg.de/pixel/lightfield_depth_estimation/" target="_blank" rel="noopener noreferrer">Project website</a>

- <a href="https://ei.is.tuebingen.mpg.de/publications/sajalalux16" target="_blank" rel="noopener noreferrer">Peer Grading in a Course on Algorithms and Data Structures: Machine Learning Algorithms do not Improve over Simple Baselines</a> (L@S 2016 Oral & ICML 2015 workshops CrowdML & ML4Ed)
  - <a href="https://www.tml.cs.uni-tuebingen.de/team/luxburg/code_and_data/peer_grading_data_request.php" target="_blank" rel="noopener noreferrer">Peer-grading dataset</a>
  - <a href="https://github.com/mortezaaa/moodle-activity_workshopplus" target="_blank" rel="noopener noreferrer">Moodle workshop plugin</a>
